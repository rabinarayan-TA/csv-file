{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "\n",
    "# standard third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# impute missing values\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import KNNImputer, IterativeImputer, SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard code-template imports\n",
    "from ta_lib.core.api import (\n",
    "    create_context, get_dataframe, get_feature_names_from_column_transformer, string_cleaning,\n",
    "    get_package_path, display_as_tabs, save_pipeline, load_pipeline, initialize_environment,\n",
    "    load_dataset, save_dataset, pd_read_from_gs, DEFAULT_ARTIFACTS_PATH, setanalyse\n",
    ")\n",
    "from ta_lib.core.io import convert_delta_to_pandas\n",
    "from ta_lib.core.api import tracker, start_experiment, update_data_run_id, init_mlflow\n",
    "import ta_lib.eda.api as eda\n",
    "from xgboost import XGBRegressor\n",
    "from ta_lib.regression.api import SKLStatsmodelOLS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ta_lib.regression.api import RegressionComparison, RegressionReport\n",
    "from ta_lib.classification.api import ClassificationComparison, ClassificationReport, confusion_matrix_by_feature, SKLStatsmodelLogit \n",
    "# from ta_lib.classification.api import metrics_classfication\n",
    "import ta_lib.reports.api as reports\n",
    "from ta_lib.data_processing.api import Outlier,WoeBinningTransformer \n",
    "\n",
    "initialize_environment(debug=False, hide_warnings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "artifacts_folder = DEFAULT_ARTIFACTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = op.join('conf', 'config.yml')\n",
    "context = create_context(config_path)\n",
    "#Rabi/scoring/notebooks/python/Feature Catalog - features_decision.csv\n",
    "decision_feature_path = op.join(os.getcwd(),'Feature Catalog - features_decision.csv')\n",
    "vif_feature_path = op.join(os.getcwd(),'vif_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_keys = ['pop_prospect_household_id','pop_prospect_household_member_no','pop_cust_id']\n",
    "ignore_cols = ['train_test_ind']\n",
    "target_col = [f\"target_{context.config['model_dev']['target']}\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "effort_id = context.config[\"model_dev\"]['effort_id']\n",
    "usecase = context.config[\"model_dev\"]['use_case']\n",
    "cohort = context.config[\"model_dev\"]['cohort']\n",
    "target = context.config['model_dev']['target']\n",
    "selection_dt = context.config['model_dev']['selection_date']\n",
    "run_date = context.config['model_dev']['run_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dec_variables(file_path):\n",
    "    df_var=pd.read_csv(file_path)\n",
    "    df_var['feature_name'] = df_var['table']+'_'+df_var['feature'].str.replace('-','_').str.replace('_+','_').str.lower()\n",
    "    dec_dict = dict(df_var[df_var.use_flag==1][['feature_name','dtype']].values)\n",
    "    return dec_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY FOR VIF FEATURES EXECUTE THE BELOW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_features_variables(file_path):\n",
    "    df_var=pd.read_csv(file_path)\n",
    "    df_var['feature_name'] = df_var['variables'].str.replace('-','_').str.replace('_+','_').str.lower()\n",
    "    dec_dict = list(df_var[df_var.use_flag==1]['feature_name'].values)\n",
    "    return dec_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Tracker (ML Flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5bf280668a12456d9bb4b2d61c5360b0\n"
     ]
    }
   ],
   "source": [
    "exp_name=context.config[\"model_dev\"][\"expt_name\"].format(use_case=usecase,cohort=cohort,target=target)\n",
    "run_name=context.config[\"model_dev\"][\"run_name\"]\n",
    "artifact_store=context.config[\"model_tracker\"][\"artifact_store\"][\"uri\"]\n",
    "mlflow_uri=context.config[\"model_tracker\"][\"backend_store\"][\"uri\"]\n",
    "\n",
    "client, expt_id = init_mlflow(exp_name, mlflow_uri, artifact_store)\n",
    "with tracker.start_run(experiment_id=expt_id,run_name = run_name) as run:\n",
    "    run_info = run.to_dictionary()\n",
    "    run_id = run_info[\"info\"][\"run_id\"]\n",
    "    print(run_id)\n",
    "    for key_ in context.config[\"model_dev\"]:\n",
    "        try:\n",
    "            key_, value_ = key_, context.config[\"model_dev\"][key_].format(**context.config[\"model_dev\"], run_id=run_id)\n",
    "        except Exception as e:\n",
    "            key_, value_ = key_, context.config[\"model_dev\"][key_]\n",
    "        tracker.log_param(key_, value_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker.start_run(run_id=run_id, experiment_id=expt_id, run_name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_dict = vif_features_variables(vif_feature_path)\n",
    "feat_dec = feature_dec_variables(decision_feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read the Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20704 uc2_postcards active opt\n"
     ]
    }
   ],
   "source": [
    "print(effort_id,usecase,cohort,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_sampled = load_dataset(context,'model_data',effort_id=effort_id,use_case=usecase,run_date=run_date,cohort=cohort,target=target)#convert_delta_to_pandas(context.config[\"model_dev\"]['gcs_bucket_name'], context.config[\"data_catalog\"]['datasets'][\"train\"][\"uri\"].format(**context.config[\"model_dev\"]),\"/home/jupyter/pch/notebooks/python/raj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid arguments provided - {'target'} :: Ignoring them ..\n"
     ]
    }
   ],
   "source": [
    "scoring_on_validation = context.config[\"model_dev\"][\"scoring_on_validation\"]\n",
    "if scoring_on_validation:\n",
    "    validation_selection_date = context.config[\"model_dev\"][\"validation_selection_date\"]\n",
    "    validation_effort_id = context.config[\"model_dev\"][\"validation_effort_id\"]\n",
    "    validation = load_dataset(context,'validation_data',effort_id=validation_effort_id,use_case=usecase,run_date=run_date,cohort=cohort,target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_sampled_clean = (\n",
    "    train_test_sampled\n",
    "    # while iterating on testing, it's good to copy the dataset(or a subset)\n",
    "    # as the following steps will mutate the input dataframe. The copy should be\n",
    "    # removed in the production code to avoid introducing perf. bottlenecks.\n",
    "    .copy()\n",
    "    #.change_type('customer_demo_cust_state_province_cd_first',str)\n",
    "    # set dtypes : nothing to do here\n",
    "    .passthrough()\n",
    "\n",
    "    .clean_names(remove_special=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    }
   ],
   "source": [
    "if scoring_on_validation:\n",
    "    print(\"Validation\")\n",
    "    validation_clean = (\n",
    "        validation\n",
    "        # while iterating on testing, it's good to copy the dataset(or a subset)\n",
    "        # as the following steps will mutate the input dataframe. The copy should be\n",
    "        # removed in the production code to avoid introducing perf. bottlenecks.\n",
    "        .copy()\n",
    "        #.change_type('customer_demo_cust_state_province_cd_first',str)\n",
    "        # set dtypes : nothing to do here\n",
    "        .passthrough()\n",
    "\n",
    "        .clean_names(remove_special=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_features=set()\n",
    "for col in feat_dec.keys():\n",
    "    if col.startswith(\"online\"):\n",
    "        online_features.add(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    50415\n",
       "m    12023\n",
       "u     2375\n",
       "Name: customer_demo_gender_cd_first, dtype: int64"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_sampled_clean[\"customer_demo_gender_cd_first\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f       37150\n",
       "m        8911\n",
       "_ukn     3187\n",
       "Name: customers_online_gender_first, dtype: int64"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_sampled_clean[\"customers_online_gender_first\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f       51337\n",
       "m       12102\n",
       "u        4474\n",
       "_ukn     3108\n",
       "0        1568\n",
       "Name: prospects_gender_cd_first, dtype: int64"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_sampled_clean[\"prospects_gender_cd_first\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols=list(train_test_sampled_clean.columns)\n",
    "train_test_sampled_clean['selection_dt'] = pd.to_datetime(selection_dt)\n",
    "if 'customer_demo_custid_create_dt_first' in list_cols:\n",
    "    train_test_sampled_clean['tenure'] = ((train_test_sampled_clean['selection_dt'] - pd.to_datetime(train_test_sampled_clean['customer_demo_custid_create_dt_first'],infer_datetime_format=True)).dt.days).apply(lambda x: max(x,0))\n",
    "\n",
    "if 'prospects_first_effort_dt_first' in list_cols:\n",
    "    train_test_sampled_clean['days_since_first_effort'] =((train_test_sampled_clean['selection_dt'] - pd.to_datetime(train_test_sampled_clean['prospects_first_effort_dt_first'],infer_datetime_format=True)).dt.days).apply(lambda x: max(x,0))\n",
    "    \n",
    "if \"customer_demo_gender_cd_first\" in list_cols:\n",
    "    train_test_sampled_clean[\"gender\"] = train_test_sampled_clean['customer_demo_gender_cd_first'] \n",
    "    train_test_sampled_clean.loc[train_test_sampled_clean[\"gender\"].isin([\"m\",\"f\",\"u\"])==False, \"gender\"] = np.nan\n",
    "    train_test_sampled_clean.drop(\"customer_demo_gender_cd_first\", axis=1, inplace=True)\n",
    "    if \"customers_online_gender_first\" in list_cols:\n",
    "        train_test_sampled_clean[\"gender\"] = train_test_sampled_clean[\"gender\"].fillna(train_test_sampled_clean['customers_online_gender_first'])\n",
    "        train_test_sampled_clean.loc[train_test_sampled_clean[\"gender\"].isin([\"m\",\"f\",\"u\"])==False, \"gender\"] = np.nan\n",
    "        train_test_sampled_clean.drop(\"customers_online_gender_first\", axis=1, inplace=True)\n",
    "        if \"prospects_gender_cd_first\" in list_cols:\n",
    "            train_test_sampled_clean[\"gender\"] = train_test_sampled_clean[\"gender\"].fillna(train_test_sampled_clean['prospects_gender_cd_first'])\n",
    "            train_test_sampled_clean.drop(\"prospects_gender_cd_first\", axis=1, inplace=True)\n",
    "            \n",
    "train_test_sampled_clean.loc[train_test_sampled_clean[\"gender\"].isin([\"m\",\"f\",\"u\"])==False, \"gender\"] = \"_ukn\"\n",
    "\n",
    "train_test_sampled_clean.drop(['customer_demo_custid_create_dt_first','prospects_first_effort_dt_first'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scoring_on_validation:\n",
    "    list_cols=list(validation_clean.columns)\n",
    "    validation_clean['selection_dt'] = pd.to_datetime(context.config[\"model_dev\"][\"validation_selection_date\"])\n",
    "    if 'customer_demo_custid_create_dt_first' in list_cols:\n",
    "        validation_clean['tenure'] = ((validation_clean['selection_dt'] - pd.to_datetime(validation_clean['customer_demo_custid_create_dt_first'],infer_datetime_format=True)).dt.days).apply(lambda x: max(x,0))\n",
    "    \n",
    "    if 'prospects_first_effort_dt_first' in list_cols:\n",
    "        validation_clean['days_since_first_effort'] =((validation_clean['selection_dt'] - pd.to_datetime(validation_clean['prospects_first_effort_dt_first'],infer_datetime_format=True)).dt.days).apply(lambda x: max(x,0))\n",
    "    \n",
    "    if \"customer_demo_gender_cd_first\" in list_cols:\n",
    "        validation_clean[\"gender\"] = validation_clean['customer_demo_gender_cd_first'] \n",
    "        validation_clean.loc[validation_clean[\"gender\"].isin([\"m\",\"f\",\"u\"])==False, \"gender\"] = np.nan\n",
    "        validation_clean.drop(\"customer_demo_gender_cd_first\", axis=1, inplace=True)\n",
    "        if \"customers_online_gender_first\" in list_cols:\n",
    "            validation_clean[\"gender\"] = validation_clean[\"gender\"].fillna(validation_clean['customers_online_gender_first'])\n",
    "            validation_clean.loc[validation_clean[\"gender\"].isin([\"m\",\"f\",\"u\"])==False, \"gender\"] = np.nan\n",
    "            validation_clean.drop(\"customers_online_gender_first\", axis=1, inplace=True)\n",
    "            if \"prospects_gender_cd_first\" in list_cols:\n",
    "                validation_clean[\"gender\"] = validation_clean[\"gender\"].fillna(validation_clean['prospects_gender_cd_first'])\n",
    "                validation_clean.drop(\"prospects_gender_cd_first\", axis=1, inplace=True)\n",
    "\n",
    "    validation_clean.loc[validation_clean[\"gender\"].isin([\"m\",\"f\",\"u\"])==False, \"gender\"] = \"_ukn\"\n",
    "\n",
    "    validation_clean.drop(['customer_demo_custid_create_dt_first','prospects_first_effort_dt_first'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f       64443\n",
       "_ukn    15746\n",
       "m       15013\n",
       "u        4405\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_sampled_clean[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 933)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(online_features),len(dec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_feature=set(dec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imp_feature)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for col in online_features:\n",
    "    imp_feature.add(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imp_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(setanalyse(list(train_test_sampled_clean.columns),list(imp_feature),simplify=False)['A^B'])\n",
    "#feature_cols = feature_cols + [\"tenure\", \"days_since_first_effort\", \"gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99607, 3457)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_sampled_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.969852\n",
       "1    0.030148\n",
       "Name: target_opt, dtype: float64"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_sampled_clean[target_col].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_clean.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31151, 3485)\n",
      "0.0    0.927193\n",
      "1.0    0.072807\n",
      "Name: target_opt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if scoring_on_validation:\n",
    "    print(validation_clean.shape)\n",
    "    print(validation_clean[target_col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_test_sampled_clean[train_test_sampled_clean.train_test_ind=='train']\n",
    "train_X = train.drop(ignore_cols+[target_col],axis=1)\n",
    "\n",
    "#select features\n",
    "train_X = train_X\n",
    "train_y = train[index_keys+[target_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_test_sampled_clean[train_test_sampled_clean.train_test_ind=='test']\n",
    "test_X = test.drop(ignore_cols+[target_col],axis=1)\n",
    "\n",
    "test_X = test_X\n",
    "test_y = test[index_keys+[target_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79740, 3455)\n",
      "(79740, 3455)\n"
     ]
    }
   ],
   "source": [
    "outlier_transformer = Outlier(method='median') #we can use percentile, actual value, median methods as well.\n",
    "print(train_X.shape)\n",
    "train_X = outlier_transformer.fit_transform(train_X)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.set_index(index_keys,inplace=True)\n",
    "train_y.set_index(index_keys,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.set_index(index_keys,inplace=True)\n",
    "test_y.set_index(index_keys,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scoring_on_validation:\n",
    "    valid_X = validation_clean.drop([target_col],axis=1)\n",
    "    valid_y = validation_clean[index_keys+[target_col]]\n",
    "    valid_X.set_index(index_keys,inplace=True)\n",
    "    valid_y.set_index(index_keys,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null with 0\n",
    "# pay given order -9999 or something\n",
    "# tru tar and ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignoring 95% null values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nulls_95_df=(train_X.isnull().sum()/len(train_X))\n",
    "nulls_95_cols=list(nulls_95_df[nulls_95_df==1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Flagging the sparse variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#ignore_cols = ignore_cols + nulls_95_cols\n",
    "for col_ in nulls_95_cols:\n",
    "    train_X[col_] = np.where(train_X[col_].notnull,1,0)\n",
    "    test_X[col_] = np.where(train_X[col_].notnull,1,0)\n",
    "    if scoring_on_validation:\n",
    "        valid_X[col_] = np.where(valid_X[col_].notnull,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X = train_X.drop(nulls_95_cols,axis=1, errors='ignore')\n",
    "#test_X = test_X.drop(nulls_95_cols,axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_cols = setanalyse(feature_cols,ignore_cols,simplify=False)['A-B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for inactive\n",
    "#feature_cols = list(set(feature_cols).difference(set(days_since_features).union(set(rfm_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=train_X[feature_cols]\n",
    "test_X=test_X[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scoring_on_validation:\n",
    "    valid_X = valid_X[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting different types of columns for transformations\n",
    "cat_columns = train_X.select_dtypes('object').columns\n",
    "num_columns = train_X.select_dtypes('number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender'], dtype='object')"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for inactive\n",
    "import re\n",
    "days_since_features = [i for i in num_columns if 'days_since' in i or 'days_between' in i]\n",
    "days_since_features.append('tenure')\n",
    "rfm_features = [i for i in num_columns if re.search('total|last|pr.[0-9]{1,2}',i) or 'growth' in i]\n",
    "cust_id_cols = [i for i in num_columns if 'cust_id' in i]\n",
    "remaining_num = list(set(num_columns) - (set(days_since_features).union(rfm_features).union(cust_id_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_columns = ['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='99031'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"f4bb04e9-e7f4-4af8-a263-93947c0d1622\" data-root-id=\"99031\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"2efcdd89-ad65-44bb-90ca-dae7aaa0cce3\":{\"roots\":{\"references\":[{\"attributes\":{\"margin\":[0,0,0,0],\"tabs\":[{\"id\":\"99057\"}]},\"id\":\"99031\",\"type\":\"Tabs\"},{\"attributes\":{\"source\":{\"id\":\"99032\"}},\"id\":\"99055\",\"type\":\"CDSView\"},{\"attributes\":{\"columns\":[{\"id\":\"99036\"},{\"id\":\"99041\"},{\"id\":\"99046\"},{\"id\":\"99051\"}],\"editable\":true,\"height\":130,\"index_position\":null,\"margin\":[5,10,5,10],\"name\":\"gender\",\"source\":{\"id\":\"99032\"},\"view\":{\"id\":\"99055\"}},\"id\":\"99054\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"99045\"},\"field\":\"#\",\"formatter\":{\"id\":\"99044\"},\"title\":\"#\"},\"id\":\"99046\",\"type\":\"TableColumn\"},{\"attributes\":{\"format\":\"0,0.0[00000]\"},\"id\":\"99049\",\"type\":\"NumberFormatter\"},{\"attributes\":{},\"id\":\"99040\",\"type\":\"StringEditor\"},{\"attributes\":{\"client_comm_id\":\"ddb086db07ea42af9cd40d4457c9e5dd\",\"comm_id\":\"8b8c1934f84a471a87f29260da46a7be\",\"plot_id\":\"99031\"},\"id\":\"99064\",\"type\":\"panel.models.comm_manager.CommManager\"},{\"attributes\":{\"editor\":{\"id\":\"99035\"},\"field\":\"index\",\"formatter\":{\"id\":\"99033\"},\"title\":\"index\"},\"id\":\"99036\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"99059\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"99056\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"99039\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"99045\",\"type\":\"IntEditor\"},{\"attributes\":{\"child\":{\"id\":\"99054\"},\"name\":\"gender\",\"title\":\"gender\"},\"id\":\"99057\",\"type\":\"Panel\"},{\"attributes\":{},\"id\":\"99033\",\"type\":\"NumberFormatter\"},{\"attributes\":{},\"id\":\"99044\",\"type\":\"NumberFormatter\"},{\"attributes\":{},\"id\":\"99035\",\"type\":\"CellEditor\"},{\"attributes\":{\"editor\":{\"id\":\"99040\"},\"field\":\"level\",\"formatter\":{\"id\":\"99039\"},\"title\":\"level\"},\"id\":\"99041\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"99050\"},\"field\":\"%\",\"formatter\":{\"id\":\"99049\"},\"title\":\"%\"},\"id\":\"99051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"99050\",\"type\":\"NumberEditor\"},{\"attributes\":{\"data\":{\"#\":[51635,12615,11957,3533],\"%\":{\"__ndarray__\":\"Cd5KBqq45D8/2gOs8z/EPz3m6WaOMcM/hh2bT1evpj8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"index\":[0,1,2,3],\"level\":[\"f\",\"_ukn\",\"m\",\"u\"]},\"selected\":{\"id\":\"99056\"},\"selection_policy\":{\"id\":\"99059\"}},\"id\":\"99032\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"99031\",\"99064\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\n",
       "    var render_items = [{\"docid\":\"2efcdd89-ad65-44bb-90ca-dae7aaa0cce3\",\"root_ids\":[\"99031\"],\"roots\":{\"99031\":\"f4bb04e9-e7f4-4af8-a263-93947c0d1622\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "Tabs\n",
       "    [0] DataFrame(name='gender', value=  level      #...)"
      ]
     },
     "execution_count": 381,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "99031"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_col_counts = []\n",
    "for col in cat_columns:\n",
    "    temp_df = pd.concat([train_X[col].value_counts(), train_X[col].value_counts(normalize=True)], axis=1)\n",
    "    temp_df.reset_index(inplace=True)\n",
    "    temp_df.columns = [\"level\", \"#\", \"%\"]\n",
    "    cat_col_counts.append((col, temp_df))\n",
    "display_as_tabs(cat_col_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_columns = cat_columns + [\"customers_online_age_range_first\", \"gender\", \"customers_online_registration_type_first\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "outlier_transformer = Outlier(method='median') #we can use percentile, actual value, median methods as well.\n",
    "print(train_X.shape)\n",
    "train_X = outlier_transformer.fit_transform(train_X)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(online_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc_columns = cat_columns+remaining_num\n",
    "\n",
    "tgt_enc = Pipeline([\n",
    "    ('target_encoding', TargetEncoder(return_df=False)),\n",
    "])\n",
    "\n",
    "one_hot_enc = Pipeline([\n",
    "    ('onehot_encoding', OneHotEncoder(drop = 'first')),\n",
    "])\n",
    "\n",
    "scaler = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "features_transformer = ColumnTransformer([\n",
    "     \n",
    "    #('remaining',SimpleImputer(strategy='constant',fill_value = '_ukn'),['customer_demo_cust_state_province_cd_first']),\n",
    "    ## numeric columns\n",
    "    ('rfm_features', SimpleImputer(strategy='constant',fill_value=0), rfm_features),\n",
    "    \n",
    "    ('days', SimpleImputer(strategy='constant',fill_value= -1440), days_since_features),\n",
    "    \n",
    "    ('tgt_enc_sim_impt', tgt_enc, list(set(cat_columns))),\n",
    "    \n",
    "    #('min_max_scaling' , scaler , rfm_features + days_since_features )\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = get_dataframe(\n",
    "    features_transformer.fit_transform(train_X, train_y), \n",
    "    get_feature_names_from_column_transformer(features_transformer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "# As part of EDA analysis this can be modified accordingle\n",
    "curated_columns = list(set(train_X.columns)-set(ignore_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the feature pipeline\n",
    "save_pipeline(features_transformer, op.abspath(op.join(artifacts_folder, f'01_features_{run_id}.joblib')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformations like these can be utilised\n",
    "def _custom_data_transform(df, cols2keep=None):\n",
    "    \"\"\"Transformation to drop some columns in the data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        df - pd.DataFrame\n",
    "        cols2keep - columns to keep in the dataframe\n",
    "    \"\"\"\n",
    "    cols2keep = cols2keep or []\n",
    "    if len(cols2keep):\n",
    "        return (df\n",
    "                .select_columns(cols2keep))\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_select=FunctionTransformer(_custom_data_transform, kw_args={'cols2keep':curated_columns})\n",
    "train_X=cols_select.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pipeline(cols_select, op.abspath(op.join(artifacts_folder, f'02_curated_columns_{run_id}.joblib')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79740, 931)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###FEATURE SELECTION FROM RANDOM FOREST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "fe_pipeline = RandomForestClassifier(max_depth = 5 , random_state = 0).fit(train_X,train_y)\n",
    "importance = fe_pipeline.feature_importances_\n",
    "feat_imp=pd.DataFrame({'features': train_X.columns,'importances':importance})\n",
    "top100=feat_imp.nlargest(100,'importances')\n",
    "selected_features_cols = top100.features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X[selected_features_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTHER SELECTION METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Eliminator\n",
    "#from ta_lib.data_processing.api import FeatureSelector\n",
    "#from sklearn.linear_model import Lasso\n",
    "#from sklearn.feature_selection import SelectFromModel\n",
    "#fe_pipeline = FeatureSelector(Lasso(alpha=0.01, random_state = 0),selection_type=\"regularization\")\n",
    "#fe_pipeline = FeatureSelector(RandomForestClassifier(max_depth = 5, random_state = 0),selection_type=\"recursion\",selection_params={'forward': False, 'k_features': 100, 'verbose': False})\n",
    "#fe_pipeline = SelectFromModel(RandomForestClassifier(max_depth = 5 , random_state = 0),max_features = 100)\n",
    "#fe_pipeline.fit(train_X,train_y)\n",
    "#train_X = fe_pipeline.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79740, 100)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = list(train_X.columns)\n",
    "# vif = eda.calc_vif(train_X)\n",
    "# while max(vif.VIF) > 15:\n",
    "#     #removing the largest variable from VIF\n",
    "#     cols.remove(vif[(vif.VIF==vif.VIF.max())].variables.tolist()[0])\n",
    "#     vif = eda.calc_vif(train_X[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_vars = vif.query('VIF < 15').variables\n",
    "# reg_vars = list(reg_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the list of relevant columns\n",
    "save_pipeline(fe_pipeline, op.abspath(op.join(artifacts_folder, f'03_feature_selection_{run_id}.joblib')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Transfomations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = get_dataframe(\n",
    "    features_transformer.transform(test_X), \n",
    "    get_feature_names_from_column_transformer(features_transformer)\n",
    ")\n",
    "test_X = cols_select.transform(test_X)\n",
    "\n",
    "#for model based selection\n",
    "test_X = test_X[selected_features_cols]\n",
    "\n",
    "#test_X = fe_pipeline.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "#for model based selection\n",
    "if scoring_on_validation:\n",
    "    valid_X = get_dataframe(\n",
    "        features_transformer.transform(valid_X), \n",
    "        get_feature_names_from_column_transformer(features_transformer)\n",
    "    )\n",
    "    valid_X = cols_select.transform(valid_X)\n",
    "    valid_X = valid_X[selected_features_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "# if scoring_on_validation:\n",
    "#     valid_X = get_dataframe(\n",
    "#         features_transformer.transform(valid_X), \n",
    "#         get_feature_names_from_column_transformer(features_transformer)\n",
    "#     )\n",
    "#     valid_X = cols_select.transform(valid_X)\n",
    "#     valid_X = fe_pipeline.transform(valid_X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tigerml.automl import AutoML\n",
    "\n",
    "# classifier_config_dict = {\n",
    "\n",
    "# #     Classifiers\n",
    "#     'sklearn.ensemble.RandomForestClassifier': {\n",
    "#         'n_estimators': [1000, 500, 750],\n",
    "#         'criterion': [\"gini\", \"entropy\"],\n",
    "#         'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "#         'min_samples_split': np.arange(50, 600, 50),\n",
    "#         'min_samples_leaf': np.arange(50, 600, 50),\n",
    "#         'bootstrap': [True, False],\n",
    "#         'class_weight': ['balanced']\n",
    "#     },\n",
    "\n",
    "#     'sklearn.linear_model.LogisticRegression': {\n",
    "#         'penalty': [\"l1\", \"l2\"],\n",
    "#         'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "#         'dual': [True, False],\n",
    "#         'class_weight': ['balanced']\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# clf = AutoML(name='automl_test',\n",
    "#     x_train=train_X,\n",
    "#     y_train=train_y[target_col],\n",
    "#     x_test=test_X,\n",
    "#     y_test=test_y[target_col],\n",
    "#     task='classification',\n",
    "#     data_type='structured',\n",
    "#     template='Classifier',\n",
    "#     search_space=classifier_config_dict,generations=2)\n",
    "\n",
    "# clf.fit()\n",
    "# print(clf.optimiser.fitted_pipeline_)\n",
    "# print(clf.optimiser.get_top_n_pipelines(1))\n",
    "# pipeline_obj=clf.get_trained_pipeline( clf.optimiser.get_top_n_pipelines(1)['pipeline_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest model\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "# clf.fit(train_X, train_y)\n",
    "# save_pipeline(clf, op.abspath(op.join(artifacts_folder, f'04_model_pipeline_rf_{run_id}.joblib')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_report_classification(model='', train_X='', train_y='', test_X='', test_y='', refit=False, shap=False, remove_metrics=[],remove_plots=[],run_id='',validation=False,name = 'rf'):\n",
    "    class_linear_report = ClassificationReport(model=model, x_train=train_X, y_train=train_y, x_test= test_X, y_test= test_y, refit=refit)\n",
    "    for metric in remove_metrics:\n",
    "        class_linear_report.remove_metric(metric)\n",
    "    for plot_name in remove_plots:\n",
    "        class_linear_report.remove_eval_plot(plot_name)\n",
    "    if validation:\n",
    "        validation = 'validation'\n",
    "    else:\n",
    "        validation = ''\n",
    "    class_linear_report.get_report(include_shap=shap, file_path=f'classification_model_report_{name}_{validation}_{run_id}')\n",
    "    return class_linear_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_metrics = ['accuracy','balanced_accuracy','precision','recall']\n",
    "remove_plots = ['precision_recall','confusion_matrix','threshold_curve']\n",
    "\n",
    "# class_linear_report=custom_report_classification(model=clf, \n",
    "#                              train_X=train_X, \n",
    "#                              train_y=train_y, \n",
    "#                              test_X=test_X, \n",
    "#                              test_y=test_y, \n",
    "#                              refit=False,\n",
    "#                              validation=False,name = 'rf',\n",
    "#                              remove_metrics=remove_metrics,\n",
    "#                             remove_plots=remove_plots,\n",
    "#                             shap=True,\n",
    "#                             run_id=run_id\n",
    "#                             )\n",
    "# if scoring_on_validation:\n",
    "#     class_linear_report_valid=custom_report_classification(model=clf, \n",
    "#                              train_X=train_X, \n",
    "#                              train_y=train_y, \n",
    "#                              test_X=valid_X, \n",
    "#                              test_y=valid_y, \n",
    "#                              refit=False,\n",
    "#                              validation=True,name = 'rf',\n",
    "#                              remove_metrics=remove_metrics,\n",
    "#                             remove_plots=remove_plots,\n",
    "#                             shap=True,\n",
    "#                             run_id=run_id\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat_train=clf.predict_proba(train_X)[:,1]#.reshape(-1,1)\n",
    "# yhat_test=clf.predict_proba(test_X)[:,1]#.reshape(-1,1)\n",
    "# if scoring_on_validation:\n",
    "#     yhat_valid=clf.predict_proba(valid_X)[:,1]#.reshape(-1,1)\n",
    "\n",
    "# from ta_lib.classification.evaluation import metrics_class\n",
    "# train_metrics=metrics_class(train_y[target_col].values,yhat_train)\n",
    "# test_metrics=metrics_class(test_y[target_col].values,yhat_test)\n",
    "# if scoring_on_validation:\n",
    "#     valid_metrics=metrics_class(valid_y[target_col].values,yhat_valid)\n",
    "    \n",
    "# temp=class_linear_report.evaluator.gains_table()\n",
    "# top3deciles_df=temp['test'][0].nlargest(3,'lift')\n",
    "# top3deciles_df.index = ['decile_1_test','decile_2_test','decile_3_test']\n",
    "# top3deciles_test=dict(top3deciles_df['lift'])\n",
    "\n",
    "# top3deciles_df_train=temp['train'][0].nlargest(3,'lift')\n",
    "# top3deciles_df_train.index = ['decile_1_train','decile_2_train','decile_3_train']\n",
    "# top3deciles_train=dict(top3deciles_df_train['lift'])\n",
    "\n",
    "# if scoring_on_validation:\n",
    "    \n",
    "#     temp_valid=class_linear_report_valid.evaluator.gains_table()\n",
    "#     top3deciles_df_valid=temp_valid['test'][0].nlargest(3,'lift')\n",
    "#     top3deciles_df_valid.index = ['decile_1_valid','decile_2_valid','decile_3_valid']\n",
    "#     top3deciles_valid=dict(top3deciles_df_valid['lift'])\n",
    "#     validation_scored = validation_clean[index_keys]\n",
    "#     validation_scored[f\"{target}_prob\"] = yhat_valid\n",
    "#     validation_scored.to_csv(f'validation_scored_{run_id}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = valid_y.copy()\n",
    "# # data['pred'] = yhat_valid\n",
    "# data.to_csv('rf_valid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracker.log_artifact('/home/jupyter/pch/notebooks/python/rf_valid.csv', 'model_objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if scoring_on_validation:\n",
    "#     for key_,val_ in valid_metrics.items():\n",
    "#         tracker.log_metric('rf_'+key_+'_valid',val_)\n",
    "#     for key_,val_ in top3deciles_valid.items():\n",
    "#         tracker.log_metric('rf_'+key_+'_valid',val_)\n",
    "#     tracker.log_metrics(top3deciles_valid)\n",
    "#     tracker.log_artifact(op.join(os.getcwd(),f'validation_scored_{run_id}.csv'), 'validations_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key_,val_ in train_metrics.items():\n",
    "#     tracker.log_metric('rf_'+key_+'_train',val_)\n",
    "\n",
    "# for key_,val_ in test_metrics.items():\n",
    "#     tracker.log_metric('rf_'+key_+'_test',val_)\n",
    "\n",
    "# for key_,val_ in top3deciles_train.items():\n",
    "#     tracker.log_metric('rf_'+key_,val_)\n",
    "    \n",
    "# for key_,val_ in top3deciles_test.items():\n",
    "#     tracker.log_metric('rf_'+key_,val_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# lasso = LogisticRegression(solver = 'saga' , penalty = 'l1' , max_iter = 10000 , C = 0.1 ,random_state = 0 )\n",
    "# lasso.fit(train_X,train_y)\n",
    "# save_pipeline(lasso, op.abspath(op.join(artifacts_folder, f'04_model_pipeline_lasso_{run_id}.joblib')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_linear_report=custom_report_classification(model=lasso, \n",
    "#                              train_X=train_X, \n",
    "#                              train_y=train_y, \n",
    "#                              test_X=test_X, \n",
    "#                              test_y=test_y, \n",
    "#                              refit=False,\n",
    "#                              validation=False,name = 'lasso',\n",
    "#                              remove_metrics=remove_metrics,\n",
    "#                             remove_plots=remove_plots,\n",
    "#                             shap=True,\n",
    "#                             run_id=run_id\n",
    "#                             )\n",
    "# if scoring_on_validation:\n",
    "#     class_linear_report_valid=custom_report_classification(model=lasso, \n",
    "#                              train_X=train_X, \n",
    "#                              train_y=train_y, \n",
    "#                              test_X=valid_X, \n",
    "#                              test_y=valid_y, \n",
    "#                              refit=False,\n",
    "#                              validation=True,name = 'lasso',\n",
    "#                              remove_metrics=remove_metrics,\n",
    "#                             remove_plots=remove_plots,\n",
    "#                             shap=True,\n",
    "#                             run_id=run_id\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat_train=lasso.predict_proba(train_X)[:,1]#.reshape(-1,1)\n",
    "# yhat_test=lasso.predict_proba(test_X)[:,1]#.reshape(-1,1)\n",
    "# if scoring_on_validation:\n",
    "#     yhat_valid=lasso.predict_proba(valid_X)[:,1]#.reshape(-1,1)\n",
    "\n",
    "# from ta_lib.classification.evaluation import metrics_class\n",
    "# train_metrics=metrics_class(train_y[target_col].values,yhat_train)\n",
    "# test_metrics=metrics_class(test_y[target_col].values,yhat_test)\n",
    "# if scoring_on_validation:\n",
    "#     valid_metrics=metrics_class(valid_y[target_col].values,yhat_valid)\n",
    "    \n",
    "# temp=class_linear_report.evaluator.gains_table()\n",
    "# top3deciles_df=temp['test'][0].nlargest(3,'lift')\n",
    "# top3deciles_df.index = ['decile_1_test','decile_2_test','decile_3_test']\n",
    "# top3deciles_test=dict(top3deciles_df['lift'])\n",
    "\n",
    "# top3deciles_df_train=temp['train'][0].nlargest(3,'lift')\n",
    "# top3deciles_df_train.index = ['decile_1_train','decile_2_train','decile_3_train']\n",
    "# top3deciles_train=dict(top3deciles_df_train['lift'])\n",
    "\n",
    "# if scoring_on_validation:\n",
    "    \n",
    "#     temp_valid=class_linear_report_valid.evaluator.gains_table()\n",
    "#     top3deciles_df_valid=temp_valid['test'][0].nlargest(3,'lift')\n",
    "#     top3deciles_df_valid.index = ['decile_1_valid','decile_2_valid','decile_3_valid']\n",
    "#     top3deciles_valid=dict(top3deciles_df_valid['lift'])\n",
    "#     validation_scored = validation_clean[index_keys]\n",
    "#     validation_scored[f\"{target}_prob\"] = yhat_valid\n",
    "#     validation_scored.to_csv(f'validation_scored_{run_id}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = valid_y.copy()\n",
    "# data['pred'] = yhat_valid\n",
    "# data.to_csv('lasso_valid.csv')\n",
    "\n",
    "# tracker.log_artifact('/home/jupyter/pch/notebooks/python/lasso_valid.csv', 'model_objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key_,val_ in train_metrics.items():\n",
    "#     tracker.log_metric(key_+'_train',val_)\n",
    "\n",
    "# for key_,val_ in test_metrics.items():\n",
    "#     tracker.log_metric(key_+'_test',val_)\n",
    "\n",
    "# for key_,val_ in top3deciles_train.items():\n",
    "#     tracker.log_metric(key_,val_)\n",
    "    \n",
    "# for key_,val_ in top3deciles_test.items():\n",
    "#     tracker.log_metric(key_,val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.1.1-py2.py3-none-manylinux1_x86_64.whl (1.8 MB)\n",
      "\u001b[K     || 1.8 MB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/envs/ta-lib-dev/lib/python3.7/site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/ta-lib-dev/lib/python3.7/site-packages (from lightgbm) (1.19.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/ta-lib-dev/lib/python3.7/site-packages (from lightgbm) (0.35.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/ta-lib-dev/lib/python3.7/site-packages (from lightgbm) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/ta-lib-dev/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/ta-lib-dev/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "xgb = LGBMClassifier(learning_rate=0.005, max_depth=15, min_child_weight=60,\n",
    "               num_leaves=10, random_state=42)#xgb = XGBClassifier(n_estimators = 10 , max_depth = 5 , eta = 0.5 , random_state = 42)\n",
    "xgb.fit(train_X,train_y)\n",
    "save_pipeline(xgb, op.abspath(op.join(artifacts_folder, f'04_model_pipeline_xgb_{run_id}.joblib')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = xgb.feature_importances_\n",
    "feature_imp=pd.DataFrame({'features': train_X.columns,'importances':xgb.feature_importances_})\n",
    "#selected_features_cols = top100.features.tolist()\n",
    "feature_imp.to_csv(\"important_feature.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'auc_roc': 0.83, 'auc_prec_recall': 0.18, 'log_loss': 0.12, 'f1_score': 0.0}\n",
      "test: {'auc_roc': 0.82, 'auc_prec_recall': 0.18, 'log_loss': 0.12, 'f1_score': 0.0}\n",
      "valid: {'auc_roc': 0.55, 'auc_prec_recall': 0.09, 'log_loss': 0.27, 'f1_score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "yhat_train=xgb.predict_proba(train_X)[:,1]#.reshape(-1,1)\n",
    "yhat_test=xgb.predict_proba(test_X)[:,1]#.reshape(-1,1)\n",
    "if scoring_on_validation:\n",
    "    yhat_valid=xgb.predict_proba(valid_X)[:,1]#.reshape(-1,1)\n",
    "\n",
    "from ta_lib.classification.evaluation import metrics_class\n",
    "train_metrics=metrics_class(train_y[target_col].values,yhat_train)\n",
    "test_metrics=metrics_class(test_y[target_col].values,yhat_test)\n",
    "if scoring_on_validation:\n",
    "    valid_metrics=metrics_class(valid_y[target_col].values,yhat_valid)\n",
    "\n",
    "print('train:',train_metrics)\n",
    "print('test:',test_metrics)\n",
    "print('valid:',valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "NPY_ARRAY_UPDATEIFCOPY, NPY_ARRAY_INOUT_ARRAY, and NPY_ARRAY_INOUT_FARRAY are deprecated, use NPY_WRITEBACKIFCOPY, NPY_ARRAY_INOUT_ARRAY2, or NPY_ARRAY_INOUT_FARRAY2 respectively instead, and call PyArray_ResolveWritebackIfCopy before the array is deallocated, i.e. before the last call to Py_DECREF.\n",
      "UPDATEIFCOPY detected in array_dealloc.  Required call to PyArray_ResolveWritebackIfCopy or PyArray_DiscardWritebackIfCopy is missing\n",
      "2021-01-27 10:34:12 - ERROR - tigerml.model_eval.plotters.interpretation::inner::169 - Cannot compute get_feature_importances. Error - No loop matching the specified signature and casting was found for ufunc true_divide\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/core/utils/_lib.py\", line 167, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/model_eval/plotters/interpretation.py\", line 822, in get_feature_importances\n",
      "    feature_importances_, features_ = vizer._get_feature_importance(labels=X.columns)\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/core/common/feature_importance.py\", line 110, in _get_feature_importance\n",
      "    feature_importances_ /= maxv\n",
      "TypeError: No loop matching the specified signature and casting was found for ufunc true_divide\n",
      "2021-01-27 10:34:12 - ERROR - tigerml.model_eval.plotters.interpretation::inner::169 - Cannot compute get_feature_importances. Error - No loop matching the specified signature and casting was found for ufunc true_divide\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/core/utils/_lib.py\", line 167, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/model_eval/plotters/interpretation.py\", line 822, in get_feature_importances\n",
      "    feature_importances_, features_ = vizer._get_feature_importance(labels=X.columns)\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/core/common/feature_importance.py\", line 110, in _get_feature_importance\n",
      "    feature_importances_ /= maxv\n",
      "TypeError: No loop matching the specified signature and casting was found for ufunc true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:param.Warning: Nesting Layouts within a HoloMap makes it difficult to access your data or control how it appears; we recommend calling .collate() on the HoloMap in order to follow the recommended nesting structure shown in the Composing Data user guide (https://goo.gl/2YS8LJ)\n",
      "Nesting Layouts within a HoloMap makes it difficult to access your data or control how it appears; we recommend calling .collate() on the HoloMap in order to follow the recommended nesting structure shown in the Composing Data user guide (https://goo.gl/2YS8LJ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NPY_ARRAY_UPDATEIFCOPY, NPY_ARRAY_INOUT_ARRAY, and NPY_ARRAY_INOUT_FARRAY are deprecated, use NPY_WRITEBACKIFCOPY, NPY_ARRAY_INOUT_ARRAY2, or NPY_ARRAY_INOUT_FARRAY2 respectively instead, and call PyArray_ResolveWritebackIfCopy before the array is deallocated, i.e. before the last call to Py_DECREF.\n",
      "UPDATEIFCOPY detected in array_dealloc.  Required call to PyArray_ResolveWritebackIfCopy or PyArray_DiscardWritebackIfCopy is missing\n",
      "2021-01-27 10:34:35 - ERROR - tigerml.model_eval.plotters.interpretation::inner::169 - Cannot compute get_feature_importances. Error - No loop matching the specified signature and casting was found for ufunc true_divide\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/core/utils/_lib.py\", line 167, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/model_eval/plotters/interpretation.py\", line 822, in get_feature_importances\n",
      "    feature_importances_, features_ = vizer._get_feature_importance(labels=X.columns)\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/core/common/feature_importance.py\", line 110, in _get_feature_importance\n",
      "    feature_importances_ /= maxv\n",
      "TypeError: No loop matching the specified signature and casting was found for ufunc true_divide\n",
      "2021-01-27 10:34:35 - ERROR - tigerml.model_eval.plotters.interpretation::inner::169 - Cannot compute get_feature_importances. Error - No loop matching the specified signature and casting was found for ufunc true_divide\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/core/utils/_lib.py\", line 167, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/model_eval/plotters/interpretation.py\", line 822, in get_feature_importances\n",
      "    feature_importances_, features_ = vizer._get_feature_importance(labels=X.columns)\n",
      "  File \"/home/jupyter/Rabi/pch/src/ta_lib/_vendor/tigerml/core/common/feature_importance.py\", line 110, in _get_feature_importance\n",
      "    feature_importances_ /= maxv\n",
      "TypeError: No loop matching the specified signature and casting was found for ufunc true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:param.Warning: Nesting Layouts within a HoloMap makes it difficult to access your data or control how it appears; we recommend calling .collate() on the HoloMap in order to follow the recommended nesting structure shown in the Composing Data user guide (https://goo.gl/2YS8LJ)\n",
      "Nesting Layouts within a HoloMap makes it difficult to access your data or control how it appears; we recommend calling .collate() on the HoloMap in order to follow the recommended nesting structure shown in the Composing Data user guide (https://goo.gl/2YS8LJ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    }
   ],
   "source": [
    "class_linear_report=custom_report_classification(model=xgb, \n",
    "                             train_X=train_X, \n",
    "                             train_y=train_y, \n",
    "                             test_X=test_X, \n",
    "                             test_y=test_y, \n",
    "                             refit=False,\n",
    "                             validation=False,name = 'xgb',\n",
    "                             remove_metrics=remove_metrics,\n",
    "                            remove_plots=remove_plots,\n",
    "                            shap=True,\n",
    "                            run_id=run_id\n",
    "                            )\n",
    "if scoring_on_validation:\n",
    "    class_linear_report_valid=custom_report_classification(model=xgb, \n",
    "                             train_X=train_X, \n",
    "                             train_y=train_y, \n",
    "                             test_X=valid_X, \n",
    "                             test_y=valid_y, \n",
    "                             refit=False,\n",
    "                             validation=True,name = 'xgb',\n",
    "                             remove_metrics=remove_metrics,\n",
    "                            remove_plots=remove_plots,\n",
    "                            shap=True,\n",
    "                            run_id=run_id\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "temp=class_linear_report.evaluator.gains_table()\n",
    "top3deciles_df=temp['test'][0].nlargest(3,'lift')\n",
    "top3deciles_df.index = ['decile_1_test','decile_2_test','decile_3_test']\n",
    "top3deciles_test=dict(top3deciles_df['lift'])\n",
    "\n",
    "top3deciles_df_train=temp['train'][0].nlargest(3,'lift')\n",
    "top3deciles_df_train.index = ['decile_1_train','decile_2_train','decile_3_train']\n",
    "top3deciles_train=dict(top3deciles_df_train['lift'])\n",
    "\n",
    "if scoring_on_validation:\n",
    "    \n",
    "    temp_valid=class_linear_report_valid.evaluator.gains_table()\n",
    "    top3deciles_df_valid=temp_valid['test'][0].nlargest(3,'lift')\n",
    "    top3deciles_df_valid.index = ['decile_1_valid','decile_2_valid','decile_3_valid']\n",
    "    top3deciles_valid=dict(top3deciles_df_valid['lift'])\n",
    "    validation_scored = validation_clean[index_keys]\n",
    "    validation_scored[f\"{target}_prob\"] = yhat_valid\n",
    "    validation_scored.to_csv(f'validation_scored_{run_id}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_X.copy()\n",
    "train_data['predicted_value'] = yhat_train\n",
    "train_data['actual_value'] = np.concatenate(train_y.values)\n",
    "\n",
    "test_data = test_X.copy()\n",
    "test_data['predicted_value'] = yhat_test\n",
    "test_data['actual_value'] = np.concatenate(test_y.values)\n",
    "\n",
    "valid_data = valid_X.copy()\n",
    "valid_data['predicted_value'] = yhat_valid\n",
    "valid_data['actual_value'] = np.concatenate(valid_y.values)\n",
    "\n",
    "train_data.to_csv(f'train_data_{run_id}.csv',index=False)\n",
    "test_data.to_csv(f'test_data_{run_id}.csv',index=False)\n",
    "if scoring_on_validation:\n",
    "    valid_data.to_csv(f'valid_data_{run_id}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.log_artifact(op.join(os.getcwd(),f'train_data_{run_id}.csv'), 'validations_score')\n",
    "tracker.log_artifact(op.join(os.getcwd(),f'test_data_{run_id}.csv'), 'validations_score')\n",
    "tracker.log_artifact(op.join(os.getcwd(),f'valid_data_{run_id}.csv'), 'validations_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_,val_ in train_metrics.items():\n",
    "    tracker.log_metric(key_+'_train',val_)\n",
    "\n",
    "for key_,val_ in test_metrics.items():\n",
    "    tracker.log_metric(key_+'_test',val_)\n",
    "\n",
    "for key_,val_ in top3deciles_train.items():\n",
    "    tracker.log_metric(key_,val_)\n",
    "    \n",
    "for key_,val_ in top3deciles_test.items():\n",
    "    tracker.log_metric(key_,val_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decile_1_train': 4.425216316440049,\n",
       " 'decile_2_train': 3.250927070457355,\n",
       " 'decile_3_train': 2.6260129103145173}"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3deciles_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decile_1_test': 4.532847117321248,\n",
       " 'decile_2_test': 3.142677482730654,\n",
       " 'decile_3_test': 2.5116162145786722}"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3deciles_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc_roc': 0.55, 'auc_prec_recall': 0.09, 'log_loss': 0.27, 'f1_score': 0.0}\n",
      "{'decile_1_valid': 1.6490829207255102, 'decile_2_valid': 1.1985981478178729, 'decile_3_valid': 1.1609090232609303}\n"
     ]
    }
   ],
   "source": [
    "if scoring_on_validation:\n",
    "    print(valid_metrics)\n",
    "    print(top3deciles_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loging and Finishing (ML Flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifacts logging\n",
    "files = [i for i in os.listdir(artifacts_folder) if i.endswith('joblib') and run_id in i]\n",
    "reports = [i for i in os.listdir(os.getcwd()) if i.endswith('html') and run_id in i]\n",
    "tracker.log_artifact(op.join(os.getcwd(),f'important_feature.csv'), 'feature importance')\n",
    "for file_ in files:\n",
    "    tracker.log_artifact(op.join(artifacts_folder,file_), 'model_objects')\n",
    "\n",
    "for report_ in reports:\n",
    "    tracker.log_artifact(op.join(os.getcwd(),report_), 'reports')\n",
    "\n",
    "if scoring_on_validation:\n",
    "    for key_,val_ in valid_metrics.items():\n",
    "        tracker.log_metric(key_+'_valid',val_)\n",
    "    tracker.log_metrics(top3deciles_valid)\n",
    "    tracker.log_artifact(op.join(os.getcwd(),f'validation_scored_{run_id}.csv'), 'validations_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.end_run(status='FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rabi/scoring/notebooks/python/04_model_pipeline_xgb_4d91f2f463624da3af0b8414b1e8e88a (1).joblib\n",
    "model_path=op.join(os.getcwd(),'04_model_pipeline_xgb_4d91f2f463624da3af0b8414b1e8e88a (1).joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "mo=joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method LGBMModel.get_params of LGBMClassifier(learning_rate=0.005, max_depth=15, min_child_weight=60,\n",
       "               num_leaves=10, random_state=42)>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ta-lib-dev]",
   "language": "python",
   "name": "conda-env-ta-lib-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
